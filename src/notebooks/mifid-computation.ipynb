{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MiFID computation\n",
    "\n",
    "Adapted from 'Demo MiFID metric for Dog image generation comp' (https://www.kaggle.com/wendykan/demo-mifid-metric-for-dog-image-generation-comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monet_jpg', 'monet_tfrec', 'photo_jpg', 'photo_tfrec']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "from constants import DATA_PATH, RESULTS_PATH\n",
    "print(os.listdir(DATA_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import gzip, pickle\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "class KernelEvalException(Exception):\n",
    "    pass\n",
    "\n",
    "model_params = {\n",
    "    'Inception': {\n",
    "        'name': 'Inception', \n",
    "        'imsize': 256,\n",
    "        'output_layer': 'pool_3:0', \n",
    "        'input_layer': 'ExpandDims:0',\n",
    "        'output_shape': 2048,\n",
    "        'cosine_distance_eps': 0.1\n",
    "        }\n",
    "}\n",
    "\n",
    "def create_model_graph(pth):\n",
    "    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n",
    "    # Creates graph from saved graph_def.pb.\n",
    "    with tf.compat.v1.gfile.GFile( pth, 'rb') as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString( f.read())\n",
    "        _ = tf.compat.v1.import_graph_def( graph_def, name='Pretrained_Net')\n",
    "\n",
    "def _get_inception_layer(sess):\n",
    "    \"\"\"Prepares inception net for batched usage and returns pool_3 layer. \n",
    "       Adapted from https://github.com/bioinf-jku/TTUR/issues/6#issuecomment-388902304\"\"\"\n",
    "    layername = 'pool_3:0'\n",
    "    pool3 = sess.graph.get_tensor_by_name(layername)\n",
    "    ops = pool3.graph.get_operations()\n",
    "    for op_idx, op in enumerate(ops):\n",
    "        for o in op.outputs:\n",
    "            shape = o.get_shape()\n",
    "            if shape._dims != []:\n",
    "                new_shape = []\n",
    "                for j, s in enumerate(shape):\n",
    "                    if s == 1 and j == 0:\n",
    "                        new_shape.append(None)\n",
    "                    else:\n",
    "                        new_shape.append(s)\n",
    "                o.__dict__['_shape_val'] = tf.TensorShape(new_shape)\n",
    "    return pool3\n",
    "\n",
    "def get_activations(images, sess, model_name, batch_size=1, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 256.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the disposable hardware.\n",
    "    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, 2048) that contains the\n",
    "       activations of the given tensor when feeding inception with the query tensor.\n",
    "    \"\"\"\n",
    "    inception_layer = _get_inception_layer(sess)\n",
    "    n_images = images.shape[0]\n",
    "    if batch_size > n_images:\n",
    "        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n",
    "        batch_size = n_images\n",
    "    n_batches = n_images // batch_size + 1 * (n_images % batch_size != 0)\n",
    "    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n",
    "        start = i*batch_size\n",
    "        if start+batch_size < n_images:\n",
    "            end = start+batch_size\n",
    "        else:\n",
    "            end = n_images\n",
    "                    \n",
    "        batch = images[start:end]\n",
    "        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n",
    "        pred_arr[start:end] = pred.reshape(-1, model_params[model_name]['output_shape'])\n",
    "    if verbose:\n",
    "        print(\" done\")\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "# def calculate_memorization_distance(features1, features2):\n",
    "#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n",
    "#     neigh.fit(features2) \n",
    "#     d, _ = neigh.kneighbors(features1, return_distance=True)\n",
    "#     print('d.shape=',d.shape)\n",
    "#     return np.mean(d)\n",
    "\n",
    "def normalize_rows(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    function that normalizes each row of the matrix x to have unit length.\n",
    "\n",
    "    Args:\n",
    "     ``x``: A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "     ``x``: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def cosine_distance(features1, features2):\n",
    "    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
    "    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
    "    norm_f1 = normalize_rows(features1_nozero)\n",
    "    norm_f2 = normalize_rows(features2_nozero)\n",
    "\n",
    "    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n",
    "    print('d.shape=',d.shape)\n",
    "    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n",
    "    mean_min_d = np.mean(np.min(d, axis=1))\n",
    "    print('distance=',mean_min_d)\n",
    "    return mean_min_d\n",
    "\n",
    "\n",
    "def distance_thresholding(d, eps):\n",
    "    if d < eps:\n",
    "        return d\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    print('covmean.shape=',covmean.shape)\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "def calculate_activation_statistics(images, sess, model_name, batch_size=64, verbose=False):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 255.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the available hardware.\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    \"\"\"\n",
    "    act = get_activations(images, sess, model_name, batch_size, verbose)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma, act\n",
    "    \n",
    "def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n",
    "    path = pathlib.Path(path)\n",
    "    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
    "    imsize = model_params[model_name]['imsize']\n",
    "\n",
    "    # In production we don't resize input images. This is just for demo purpose. \n",
    "    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
    "    m, s, features = calculate_activation_statistics(x, sess, model_name)\n",
    "    del x #clean up memory\n",
    "    return m, s, features\n",
    "\n",
    "# check for image size\n",
    "def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n",
    "    im = Image.open(str(filename))\n",
    "    if is_checksize and im.size != (check_imsize,check_imsize):\n",
    "        raise KernelEvalException('The images are not of size '+str(check_imsize))\n",
    "    \n",
    "    if is_check_png and im.format != 'PNG':\n",
    "        raise KernelEvalException('Only PNG images should be submitted.')\n",
    "\n",
    "    if resize_to is None:\n",
    "        return im\n",
    "    else:\n",
    "        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n",
    "\n",
    "def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None):\n",
    "    ''' Calculates the KID of two paths. '''\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    create_model_graph(str(model_path))\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = False)\n",
    "        if feature_path is None:\n",
    "            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n",
    "        else:\n",
    "            with np.load(feature_path) as f:\n",
    "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
    "\n",
    "        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n",
    "        print('starting calculating FID')\n",
    "        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "        print('done with FID, starting distance calculation')\n",
    "        distance = cosine_distance(features1, features2)        \n",
    "        return fid_value, distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 110/110 [00:31<00:00,  3.46it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m1,m2 shape= ((2048,), (2048,)) s1,s2= ((2048, 2048), (2048, 2048))\n",
      "starting calculating FID\n",
      "covmean.shape= (2048, 2048)\n",
      "done with FID, starting distance calculation\n",
      "d.shape= (7038, 300)\n",
      "np.min(d, axis=1).shape= (7038,)\n",
      "distance= 0.23662488801256065\n",
      "FID_public:  79.86430708622586 distance_public:  0.23662488801256065 distance thresholded 1 multiplied_public:  79.86430708622507\n"
     ]
    }
   ],
   "source": [
    "tf.debugging.set_log_device_placement(True)\n",
    "\n",
    "images_path = [RESULTS_PATH+'notebook_150_600_7000/',DATA_PATH+'monet_jpg/']\n",
    "\n",
    "model_path = '../models/classify_image_graph_def.pb'\n",
    "\n",
    "fid_epsilon = 10e-15\n",
    "with tf.device('/GPU:0'):\n",
    "    fid_value_public, distance = calculate_kid_given_paths(images_path, 'Inception', model_path)\n",
    "    distance_thresh = distance_thresholding(distance, model_params['Inception']['cosine_distance_eps'])\n",
    "    print(\"FID_public: \", fid_value_public, \"distance_public: \", distance, \"distance thresholded\", distance_thresh, \"multiplied_public: \", fid_value_public /(distance_thresh + fid_epsilon))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* A very bad notebook: 302.131 instead of 285.07819\n",
    "* Input images: 117.558 instead of 78.96105\n",
    "* Simple Cycle GAN: 85.468 instead of 49.37787\n",
    "* Best notebook: 73.887 instead of 38.38419"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.06 17.053\n",
      "1.489 38.597\n",
      "1.731 36.09\n",
      "1.925 35.503\n"
     ]
    }
   ],
   "source": [
    "print(round(302.131 / 285.07819, 3), round(302.131 - 285.07819, 3))\n",
    "print(round(117.558 / 78.96105, 3), round(117.558 - 78.96105, 3))\n",
    "print(round(85.468 / 49.37787, 3), round(85.468 - 49.37787, 3))\n",
    "print(round(73.887 / 38.38419, 3), round(73.887 - 38.38419, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "notebook_100_600_14000: 79.86431158679329\n",
    "\n",
    "notebook_150_600_7000:  79.864311480927"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AxesSubplot' object has no attribute 'rcParams'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-f6265f3b003d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrcParams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'font.size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'16'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m285.07819\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m78.96105\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m49.37787\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m38.3841\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Kaggle MiFID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mbar_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m302.131\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m117.558\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m85.468\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m73.887\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbar_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Locally computed MiFID\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AxesSubplot' object has no attribute 'rcParams'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 648x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "bar_width = 0.2\n",
    "\n",
    "x = np.array([0,1,2,3])\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "ax.rcParams['font.size'] = '16'\n",
    "ax.bar([0, 1, 2, 3], [285.07819, 78.96105, 49.37787, 38.3841], width=bar_width, label=\"Kaggle MiFID\")\n",
    "ax.bar(x + bar_width, [302.131, 117.558, 85.468, 73.887], width=bar_width, label=\"Locally computed MiFID\")\n",
    "ax.set_xticks(x + bar_width / 2)\n",
    "ax.set_xticklabels(x)\n",
    "ax.legend()\n",
    "ax.savefig(\"mifid.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
