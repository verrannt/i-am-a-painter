{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['monet_jpg', 'monet_tfrec', 'photo_jpg', 'photo_tfrec']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from constants import DATA_PATH\n",
    "print(os.listdir(DATA_PATH))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "import numpy as np\n",
    "import os\n",
    "import gzip, pickle\n",
    "import tensorflow as tf\n",
    "from scipy import linalg\n",
    "import pathlib\n",
    "import urllib\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "class KernelEvalException(Exception):\n",
    "    pass\n",
    "\n",
    "model_params = {\n",
    "    'Inception': {\n",
    "        'name': 'Inception', \n",
    "        'imsize': 256,\n",
    "        'output_layer': 'pool_3:0', \n",
    "        'input_layer': 'ExpandDims:0',\n",
    "        'output_shape': 2048,\n",
    "        'cosine_distance_eps': 0.1\n",
    "        }\n",
    "}\n",
    "\n",
    "def create_model_graph(pth):\n",
    "    \"\"\"Creates a graph from saved GraphDef file.\"\"\"\n",
    "    # Creates graph from saved graph_def.pb.\n",
    "    with tf.compat.v1.gfile.FastGFile( pth, 'rb') as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString( f.read())\n",
    "        _ = tf.compat.v1.import_graph_def( graph_def, name='Pretrained_Net')\n",
    "\n",
    "def _get_model_layer(sess, model_name):\n",
    "    # layername = 'Pretrained_Net/final_layer/Mean:0'\n",
    "    layername = model_params[model_name]['output_layer']\n",
    "    print([tensor for op in tf.compat.v1.get_default_graph().get_operations() for tensor in op.values()])\n",
    "    layer = sess.graph.get_tensor_by_name(layername)\n",
    "#     ops = layer.graph.get_operations()\n",
    "#     for op_idx, op in enumerate(ops):\n",
    "#         for o in op.outputs:\n",
    "#             shape = o.get_shape()\n",
    "#             print(shape)\n",
    "#             if shape._dims != []:\n",
    "#                 shape = [s.value for s in shape]\n",
    "#                 new_shape = []\n",
    "#             for j, s in enumerate(shape):\n",
    "#                 if s == 1 and j == 0:\n",
    "#                     new_shape.append(None)\n",
    "#                 else:\n",
    "#                     new_shape.append(s)\n",
    "#                 o.__dict__['_shape_val'] = tf.compat.v1.TensorShape(new_shape)\n",
    "    return layer\n",
    "\n",
    "def get_activations(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculates the activations of the pool_3 layer for all images.\n",
    "\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 256.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the disposable hardware.\n",
    "    -- verbose    : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- A numpy array of dimension (num images, 2048) that contains the\n",
    "       activations of the given tensor when feeding inception with the query tensor.\n",
    "    \"\"\"\n",
    "    inception_layer = _get_model_layer(sess, model_name)\n",
    "    n_images = images.shape[0]\n",
    "    if batch_size > n_images:\n",
    "        print(\"warning: batch size is bigger than the data size. setting batch size to data size\")\n",
    "        batch_size = n_images\n",
    "    n_batches = n_images//batch_size + 1\n",
    "    pred_arr = np.empty((n_images,model_params[model_name]['output_shape']))\n",
    "    for i in tqdm(range(n_batches)):\n",
    "        if verbose:\n",
    "            print(\"\\rPropagating batch %d/%d\" % (i+1, n_batches), end=\"\", flush=True)\n",
    "        start = i*batch_size\n",
    "        if start+batch_size < n_images:\n",
    "            end = start+batch_size\n",
    "        else:\n",
    "            end = n_images\n",
    "                    \n",
    "        batch = images[start:end]\n",
    "        pred = sess.run(inception_layer, {model_params[model_name]['input_layer']: batch})\n",
    "        pred_arr[start:end] = pred.reshape(-1,model_params[model_name]['output_shape'])\n",
    "    if verbose:\n",
    "        print(\" done\")\n",
    "    return pred_arr\n",
    "\n",
    "\n",
    "# def calculate_memorization_distance(features1, features2):\n",
    "#     neigh = NearestNeighbors(n_neighbors=1, algorithm='kd_tree', metric='euclidean')\n",
    "#     neigh.fit(features2) \n",
    "#     d, _ = neigh.kneighbors(features1, return_distance=True)\n",
    "#     print('d.shape=',d.shape)\n",
    "#     return np.mean(d)\n",
    "\n",
    "def normalize_rows(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    function that normalizes each row of the matrix x to have unit length.\n",
    "\n",
    "    Args:\n",
    "     ``x``: A numpy matrix of shape (n, m)\n",
    "\n",
    "    Returns:\n",
    "     ``x``: The normalized (by row) numpy matrix.\n",
    "    \"\"\"\n",
    "    return np.nan_to_num(x/np.linalg.norm(x, ord=2, axis=1, keepdims=True))\n",
    "\n",
    "\n",
    "def cosine_distance(features1, features2):\n",
    "    features1_nozero = features1[np.sum(features1, axis=1) != 0]\n",
    "    features2_nozero = features2[np.sum(features2, axis=1) != 0]\n",
    "    norm_f1 = normalize_rows(features1_nozero)\n",
    "    norm_f2 = normalize_rows(features2_nozero)\n",
    "\n",
    "    d = 1.0-np.abs(np.matmul(norm_f1, norm_f2.T))\n",
    "    print('d.shape=',d.shape)\n",
    "    print('np.min(d, axis=1).shape=',np.min(d, axis=1).shape)\n",
    "    mean_min_d = np.mean(np.min(d, axis=1))\n",
    "    print('distance=',mean_min_d)\n",
    "    return mean_min_d\n",
    "\n",
    "\n",
    "def distance_thresholding(d, eps):\n",
    "    if d < eps:\n",
    "        return d\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "            \n",
    "    Stable version by Dougal J. Sutherland.\n",
    "\n",
    "    Params:\n",
    "    -- mu1 : Numpy array containing the activations of the pool_3 layer of the\n",
    "             inception net ( like returned by the function 'get_predictions')\n",
    "             for generated samples.\n",
    "    -- mu2   : The sample mean over activations of the pool_3 layer, precalcualted\n",
    "               on an representive data set.\n",
    "    -- sigma1: The covariance matrix over activations of the pool_3 layer for\n",
    "               generated samples.\n",
    "    -- sigma2: The covariance matrix over activations of the pool_3 layer,\n",
    "               precalcualted on an representive data set.\n",
    "\n",
    "    Returns:\n",
    "    --   : The Frechet Distance.\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \"Training and test mean vectors have different lengths\"\n",
    "    assert sigma1.shape == sigma2.shape, \"Training and test covariances have different dimensions\"\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    # product might be almost singular\n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = \"fid calculation produces singular product; adding %s to diagonal of cov estimates\" % eps\n",
    "        warnings.warn(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "    \n",
    "    # numerical error might give slight imaginary component\n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError(\"Imaginary component {}\".format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    print('covmean.shape=',covmean.shape)\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "    return diff.dot(diff) + np.trace(sigma1) + np.trace(sigma2) - 2 * tr_covmean\n",
    "\n",
    "def calculate_activation_statistics(images, sess, model_name, batch_size=50, verbose=False):\n",
    "    \"\"\"Calculation of the statistics used by the FID.\n",
    "    Params:\n",
    "    -- images      : Numpy array of dimension (n_images, hi, wi, 3). The values\n",
    "                     must lie between 0 and 255.\n",
    "    -- sess        : current session\n",
    "    -- batch_size  : the images numpy array is split into batches with batch size\n",
    "                     batch_size. A reasonable batch size depends on the available hardware.\n",
    "    -- verbose     : If set to True and parameter out_step is given, the number of calculated\n",
    "                     batches is reported.\n",
    "    Returns:\n",
    "    -- mu    : The mean over samples of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    -- sigma : The covariance matrix of the activations of the pool_3 layer of\n",
    "               the incption model.\n",
    "    \"\"\"\n",
    "    act = get_activations(images, sess, model_name, batch_size, verbose)\n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma, act\n",
    "    \n",
    "def _handle_path_memorization(path, sess, model_name, is_checksize, is_check_png):\n",
    "    path = pathlib.Path(path)\n",
    "    files = list(path.glob('*.jpg')) + list(path.glob('*.png'))\n",
    "    imsize = model_params[model_name]['imsize']\n",
    "\n",
    "    # In production we don't resize input images. This is just for demo purpose. \n",
    "    x = np.array([np.array(img_read_checks(fn, imsize, is_checksize, imsize, is_check_png)) for fn in files])\n",
    "    m, s, features = calculate_activation_statistics(x, sess, model_name)\n",
    "    del x #clean up memory\n",
    "    return m, s, features\n",
    "\n",
    "# check for image size\n",
    "def img_read_checks(filename, resize_to, is_checksize=False, check_imsize = 64, is_check_png = False):\n",
    "    im = Image.open(str(filename))\n",
    "    if is_checksize and im.size != (check_imsize,check_imsize):\n",
    "        raise KernelEvalException('The images are not of size '+str(check_imsize))\n",
    "    \n",
    "    if is_check_png and im.format != 'PNG':\n",
    "        raise KernelEvalException('Only PNG images should be submitted.')\n",
    "\n",
    "    if resize_to is None:\n",
    "        return im\n",
    "    else:\n",
    "        return im.resize((resize_to,resize_to),Image.ANTIALIAS)\n",
    "\n",
    "def calculate_kid_given_paths(paths, model_name, model_path, feature_path=None):\n",
    "    ''' Calculates the KID of two paths. '''\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    create_model_graph(str(model_path))\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(tf.compat.v1.global_variables_initializer())\n",
    "        m1, s1, features1 = _handle_path_memorization(paths[0], sess, model_name, is_checksize = True, is_check_png = False)\n",
    "        if feature_path is None:\n",
    "            m2, s2, features2 = _handle_path_memorization(paths[1], sess, model_name, is_checksize = False, is_check_png = False)\n",
    "        else:\n",
    "            with np.load(feature_path) as f:\n",
    "                m2, s2, features2 = f['m'], f['s'], f['features']\n",
    "\n",
    "        print('m1,m2 shape=',(m1.shape,m2.shape),'s1,s2=',(s1.shape,s2.shape))\n",
    "        print('starting calculating FID')\n",
    "        fid_value = calculate_frechet_distance(m1, s1, m2, s2)\n",
    "        print('done with FID, starting distance calculation')\n",
    "        distance = cosine_distance(features1, features2)        \n",
    "        return fid_value, distance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                          | 0/141 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'DecodeJpeg/contents:0' shape=() dtype=string>, <tf.Tensor 'DecodeJpeg:0' shape=(None, None, 3) dtype=uint8>, <tf.Tensor 'Cast:0' shape=(None, None, 3) dtype=float32>, <tf.Tensor 'ExpandDims/dim:0' shape=(1,) dtype=int32>, <tf.Tensor 'ExpandDims:0' shape=(1, None, None, 3) dtype=float32>, <tf.Tensor 'ResizeBilinear/size:0' shape=(2,) dtype=int32>, <tf.Tensor 'ResizeBilinear:0' shape=(1, 299, 299, 3) dtype=float32>, <tf.Tensor 'Sub/y:0' shape=() dtype=float32>, <tf.Tensor 'Sub:0' shape=(1, 299, 299, 3) dtype=float32>, <tf.Tensor 'Mul/y:0' shape=() dtype=float32>, <tf.Tensor 'Mul:0' shape=(1, 299, 299, 3) dtype=float32>, <tf.Tensor 'conv/conv2d_params:0' shape=(3, 3, 3, 32) dtype=float32>, <tf.Tensor 'conv/Conv2D:0' shape=(1, 149, 149, 32) dtype=float32>, <tf.Tensor 'conv/batchnorm/beta:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv/batchnorm/gamma:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv/batchnorm/moving_mean:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv/batchnorm/moving_variance:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv/batchnorm:0' shape=(1, 149, 149, 32) dtype=float32>, <tf.Tensor 'conv/CheckNumerics:0' shape=(1, 149, 149, 32) dtype=float32>, <tf.Tensor 'conv/control_dependency:0' shape=(1, 149, 149, 32) dtype=float32>, <tf.Tensor 'conv:0' shape=(1, 149, 149, 32) dtype=float32>, <tf.Tensor 'conv_1/conv2d_params:0' shape=(3, 3, 32, 32) dtype=float32>, <tf.Tensor 'conv_1/Conv2D:0' shape=(1, 147, 147, 32) dtype=float32>, <tf.Tensor 'conv_1/batchnorm/beta:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv_1/batchnorm/gamma:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv_1/batchnorm/moving_mean:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv_1/batchnorm/moving_variance:0' shape=(32,) dtype=float32>, <tf.Tensor 'conv_1/batchnorm:0' shape=(1, 147, 147, 32) dtype=float32>, <tf.Tensor 'conv_1/CheckNumerics:0' shape=(1, 147, 147, 32) dtype=float32>, <tf.Tensor 'conv_1/control_dependency:0' shape=(1, 147, 147, 32) dtype=float32>, <tf.Tensor 'conv_1:0' shape=(1, 147, 147, 32) dtype=float32>, <tf.Tensor 'conv_2/conv2d_params:0' shape=(3, 3, 32, 64) dtype=float32>, <tf.Tensor 'conv_2/Conv2D:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'conv_2/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'conv_2/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'conv_2/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'conv_2/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'conv_2/batchnorm:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'conv_2/CheckNumerics:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'conv_2/control_dependency:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'conv_2:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'pool/CheckNumerics:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'pool/control_dependency:0' shape=(1, 147, 147, 64) dtype=float32>, <tf.Tensor 'pool:0' shape=(1, 73, 73, 64) dtype=float32>, <tf.Tensor 'conv_3/conv2d_params:0' shape=(1, 1, 64, 80) dtype=float32>, <tf.Tensor 'conv_3/Conv2D:0' shape=(1, 73, 73, 80) dtype=float32>, <tf.Tensor 'conv_3/batchnorm/beta:0' shape=(80,) dtype=float32>, <tf.Tensor 'conv_3/batchnorm/gamma:0' shape=(80,) dtype=float32>, <tf.Tensor 'conv_3/batchnorm/moving_mean:0' shape=(80,) dtype=float32>, <tf.Tensor 'conv_3/batchnorm/moving_variance:0' shape=(80,) dtype=float32>, <tf.Tensor 'conv_3/batchnorm:0' shape=(1, 73, 73, 80) dtype=float32>, <tf.Tensor 'conv_3/CheckNumerics:0' shape=(1, 73, 73, 80) dtype=float32>, <tf.Tensor 'conv_3/control_dependency:0' shape=(1, 73, 73, 80) dtype=float32>, <tf.Tensor 'conv_3:0' shape=(1, 73, 73, 80) dtype=float32>, <tf.Tensor 'conv_4/conv2d_params:0' shape=(3, 3, 80, 192) dtype=float32>, <tf.Tensor 'conv_4/Conv2D:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'conv_4/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'conv_4/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'conv_4/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'conv_4/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'conv_4/batchnorm:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'conv_4/CheckNumerics:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'conv_4/control_dependency:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'conv_4:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'pool_1/CheckNumerics:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'pool_1/control_dependency:0' shape=(1, 71, 71, 192) dtype=float32>, <tf.Tensor 'pool_1:0' shape=(1, 35, 35, 192) dtype=float32>, <tf.Tensor 'mixed/conv/conv2d_params:0' shape=(1, 1, 192, 64) dtype=float32>, <tf.Tensor 'mixed/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv/conv2d_params:0' shape=(1, 1, 192, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv/Conv2D:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv/batchnorm/beta:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed/tower/conv/batchnorm/gamma:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed/tower/conv/batchnorm/moving_mean:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed/tower/conv/batchnorm/moving_variance:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed/tower/conv/batchnorm:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv/CheckNumerics:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv/control_dependency:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/conv2d_params:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower/conv_1:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/conv2d_params:0' shape=(1, 1, 192, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/conv2d_params:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_1:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/conv2d_params:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_1/conv_2:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed/tower_2/pool:0' shape=(1, 35, 35, 192) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/conv2d_params:0' shape=(1, 1, 192, 32) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/Conv2D:0' shape=(1, 35, 35, 32) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/batchnorm/beta:0' shape=(32,) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/batchnorm/gamma:0' shape=(32,) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/batchnorm/moving_mean:0' shape=(32,) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/batchnorm/moving_variance:0' shape=(32,) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/batchnorm:0' shape=(1, 35, 35, 32) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/CheckNumerics:0' shape=(1, 35, 35, 32) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv/control_dependency:0' shape=(1, 35, 35, 32) dtype=float32>, <tf.Tensor 'mixed/tower_2/conv:0' shape=(1, 35, 35, 32) dtype=float32>, <tf.Tensor 'mixed/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed/join:0' shape=(1, 35, 35, 256) dtype=float32>, <tf.Tensor 'mixed_1/conv/conv2d_params:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Tensor 'mixed_1/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/conv2d_params:0' shape=(1, 1, 256, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/Conv2D:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/batchnorm/beta:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/batchnorm/gamma:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/batchnorm/moving_mean:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/batchnorm/moving_variance:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/batchnorm:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/CheckNumerics:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv/control_dependency:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/conv2d_params:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower/conv_1:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/conv2d_params:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/conv2d_params:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_1:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/conv2d_params:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_1/conv_2:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/pool:0' shape=(1, 35, 35, 256) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/conv2d_params:0' shape=(1, 1, 256, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/tower_2/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_1/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_1/join:0' shape=(1, 35, 35, 288) dtype=float32>, <tf.Tensor 'mixed_2/conv/conv2d_params:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Tensor 'mixed_2/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/conv2d_params:0' shape=(1, 1, 288, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/Conv2D:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/batchnorm/beta:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/batchnorm/gamma:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/batchnorm/moving_mean:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/batchnorm/moving_variance:0' shape=(48,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/batchnorm:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/CheckNumerics:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv/control_dependency:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv:0' shape=(1, 35, 35, 48) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/conv2d_params:0' shape=(5, 5, 48, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower/conv_1:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/conv2d_params:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/conv2d_params:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_1:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/conv2d_params:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_1/conv_2:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/pool:0' shape=(1, 35, 35, 288) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/conv2d_params:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/tower_2/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_2/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_2/join:0' shape=(1, 35, 35, 288) dtype=float32>, <tf.Tensor 'mixed_3/conv/conv2d_params:0' shape=(3, 3, 288, 384) dtype=float32>, <tf.Tensor 'mixed_3/conv/Conv2D:0' shape=(1, 17, 17, 384) dtype=float32>, <tf.Tensor 'mixed_3/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_3/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_3/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_3/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_3/conv/batchnorm:0' shape=(1, 17, 17, 384) dtype=float32>, <tf.Tensor 'mixed_3/conv/CheckNumerics:0' shape=(1, 17, 17, 384) dtype=float32>, <tf.Tensor 'mixed_3/conv/control_dependency:0' shape=(1, 17, 17, 384) dtype=float32>, <tf.Tensor 'mixed_3/conv:0' shape=(1, 17, 17, 384) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/conv2d_params:0' shape=(1, 1, 288, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/Conv2D:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/batchnorm/beta:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/batchnorm/gamma:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/batchnorm/moving_mean:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/batchnorm/moving_variance:0' shape=(64,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/batchnorm:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/CheckNumerics:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv/control_dependency:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv:0' shape=(1, 35, 35, 64) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/conv2d_params:0' shape=(3, 3, 64, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/Conv2D:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/batchnorm:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/CheckNumerics:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1/control_dependency:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_1:0' shape=(1, 35, 35, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/conv2d_params:0' shape=(3, 3, 96, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/Conv2D:0' shape=(1, 17, 17, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/batchnorm/beta:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/batchnorm/gamma:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/batchnorm/moving_mean:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/batchnorm/moving_variance:0' shape=(96,) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/batchnorm:0' shape=(1, 17, 17, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/CheckNumerics:0' shape=(1, 17, 17, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2/control_dependency:0' shape=(1, 17, 17, 96) dtype=float32>, <tf.Tensor 'mixed_3/tower/conv_2:0' shape=(1, 17, 17, 96) dtype=float32>, <tf.Tensor 'mixed_3/pool/CheckNumerics:0' shape=(1, 35, 35, 288) dtype=float32>, <tf.Tensor 'mixed_3/pool/control_dependency:0' shape=(1, 35, 35, 288) dtype=float32>, <tf.Tensor 'mixed_3/pool:0' shape=(1, 17, 17, 288) dtype=float32>, <tf.Tensor 'mixed_3/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_3/join:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_4/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_4/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/conv2d_params:0' shape=(1, 1, 768, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/conv2d_params:0' shape=(1, 7, 128, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_1:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/conv2d_params:0' shape=(7, 1, 128, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/conv2d_params:0' shape=(1, 1, 768, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/conv2d_params:0' shape=(7, 1, 128, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_1:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/conv2d_params:0' shape=(1, 7, 128, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_2:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/conv2d_params:0' shape=(7, 1, 128, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/Conv2D:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/batchnorm/beta:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/batchnorm/gamma:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/batchnorm/moving_mean:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/batchnorm/moving_variance:0' shape=(128,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/batchnorm:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/CheckNumerics:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3/control_dependency:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_3:0' shape=(1, 17, 17, 128) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/conv2d_params:0' shape=(1, 7, 128, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_1/conv_4:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/pool:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/tower_2/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_4/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_4/join:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_5/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_5/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/conv2d_params:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/conv2d_params:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_1:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/conv2d_params:0' shape=(7, 1, 160, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/conv2d_params:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/conv2d_params:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_1:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/conv2d_params:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_2:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/conv2d_params:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_3:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/conv2d_params:0' shape=(1, 7, 160, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_1/conv_4:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/pool:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/tower_2/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_5/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_5/join:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_6/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_6/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/conv2d_params:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/conv2d_params:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_1:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/conv2d_params:0' shape=(7, 1, 160, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/conv2d_params:0' shape=(1, 1, 768, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/conv2d_params:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_1:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/conv2d_params:0' shape=(1, 7, 160, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_2:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/conv2d_params:0' shape=(7, 1, 160, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/Conv2D:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/batchnorm/beta:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/batchnorm/gamma:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/batchnorm/moving_mean:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/batchnorm/moving_variance:0' shape=(160,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/batchnorm:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/CheckNumerics:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3/control_dependency:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_3:0' shape=(1, 17, 17, 160) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/conv2d_params:0' shape=(1, 7, 160, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_1/conv_4:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/pool:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/tower_2/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_6/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_6/join:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_7/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_7/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/conv2d_params:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_1:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/conv2d_params:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/conv2d_params:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_1:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/conv2d_params:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/conv2d_params:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_3:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/conv2d_params:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_1/conv_4:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/pool:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/tower_2/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_7/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_7/join:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/conv2d_params:0' shape=(3, 3, 192, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/Conv2D:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/batchnorm/beta:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/batchnorm/gamma:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/batchnorm/moving_mean:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/batchnorm/moving_variance:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/batchnorm:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/CheckNumerics:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1/control_dependency:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower/conv_1:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/conv2d_params:0' shape=(1, 1, 768, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/conv2d_params:0' shape=(1, 7, 192, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_1:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/conv2d_params:0' shape=(7, 1, 192, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/Conv2D:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/batchnorm:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/CheckNumerics:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2/control_dependency:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_2:0' shape=(1, 17, 17, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/conv2d_params:0' shape=(3, 3, 192, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/Conv2D:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/batchnorm:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/CheckNumerics:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3/control_dependency:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_8/tower_1/conv_3:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_8/pool/CheckNumerics:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_8/pool/control_dependency:0' shape=(1, 17, 17, 768) dtype=float32>, <tf.Tensor 'mixed_8/pool:0' shape=(1, 8, 8, 768) dtype=float32>, <tf.Tensor 'mixed_8/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_8/join:0' shape=(1, 8, 8, 1280) dtype=float32>, <tf.Tensor 'mixed_9/conv/conv2d_params:0' shape=(1, 1, 1280, 320) dtype=float32>, <tf.Tensor 'mixed_9/conv/Conv2D:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_9/conv/batchnorm/beta:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_9/conv/batchnorm/gamma:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_9/conv/batchnorm/moving_mean:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_9/conv/batchnorm/moving_variance:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_9/conv/batchnorm:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_9/conv/CheckNumerics:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_9/conv/control_dependency:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_9/conv:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/conv2d_params:0' shape=(1, 1, 1280, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/conv2d_params:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/conv2d_params:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower/mixed/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/conv2d_params:0' shape=(1, 1, 1280, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/Conv2D:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/batchnorm/beta:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/batchnorm/gamma:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/batchnorm/moving_mean:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/batchnorm/moving_variance:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/batchnorm:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/CheckNumerics:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv/control_dependency:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/conv2d_params:0' shape=(3, 3, 448, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/conv2d_params:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/conv2d_params:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_1/mixed/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/pool:0' shape=(1, 8, 8, 1280) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/conv2d_params:0' shape=(1, 1, 1280, 192) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/Conv2D:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/batchnorm:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/CheckNumerics:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv/control_dependency:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_9/tower_2/conv:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_9/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_9/join:0' shape=(1, 8, 8, 2048) dtype=float32>, <tf.Tensor 'mixed_10/conv/conv2d_params:0' shape=(1, 1, 2048, 320) dtype=float32>, <tf.Tensor 'mixed_10/conv/Conv2D:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_10/conv/batchnorm/beta:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_10/conv/batchnorm/gamma:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_10/conv/batchnorm/moving_mean:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_10/conv/batchnorm/moving_variance:0' shape=(320,) dtype=float32>, <tf.Tensor 'mixed_10/conv/batchnorm:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_10/conv/CheckNumerics:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_10/conv/control_dependency:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_10/conv:0' shape=(1, 8, 8, 320) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/conv2d_params:0' shape=(1, 1, 2048, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/conv2d_params:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/conv2d_params:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower/mixed/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/conv2d_params:0' shape=(1, 1, 2048, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/Conv2D:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/batchnorm/beta:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/batchnorm/gamma:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/batchnorm/moving_mean:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/batchnorm/moving_variance:0' shape=(448,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/batchnorm:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/CheckNumerics:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv/control_dependency:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv:0' shape=(1, 8, 8, 448) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/conv2d_params:0' shape=(3, 3, 448, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/conv2d_params:0' shape=(1, 3, 384, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/conv2d_params:0' shape=(3, 1, 384, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/Conv2D:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/batchnorm/beta:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/batchnorm/gamma:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/batchnorm/moving_mean:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/batchnorm/moving_variance:0' shape=(384,) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/batchnorm:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/CheckNumerics:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1/control_dependency:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_1/mixed/conv_1:0' shape=(1, 8, 8, 384) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/pool/CheckNumerics:0' shape=(1, 8, 8, 2048) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/pool/control_dependency:0' shape=(1, 8, 8, 2048) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/pool:0' shape=(1, 8, 8, 2048) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/conv2d_params:0' shape=(1, 1, 2048, 192) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/Conv2D:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/batchnorm/beta:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/batchnorm/gamma:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/batchnorm/moving_mean:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/batchnorm/moving_variance:0' shape=(192,) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/batchnorm:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/CheckNumerics:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv/control_dependency:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_10/tower_2/conv:0' shape=(1, 8, 8, 192) dtype=float32>, <tf.Tensor 'mixed_10/join/concat_dim:0' shape=() dtype=int32>, <tf.Tensor 'mixed_10/join:0' shape=(1, 8, 8, 2048) dtype=float32>, <tf.Tensor 'pool_3:0' shape=(1, 1, 1, 2048) dtype=float32>, <tf.Tensor 'pool_3/_reshape/shape:0' shape=(2,) dtype=int32>, <tf.Tensor 'pool_3/_reshape:0' shape=(1, 2048) dtype=float32>, <tf.Tensor 'softmax/weights:0' shape=(2048, 1008) dtype=float32>, <tf.Tensor 'softmax/biases:0' shape=(1008,) dtype=float32>, <tf.Tensor 'softmax/logits/MatMul:0' shape=(1, 1008) dtype=float32>, <tf.Tensor 'softmax/logits:0' shape=(1, 1008) dtype=float32>, <tf.Tensor 'softmax:0' shape=(1, 1008) dtype=float32>]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (50, 256, 256, 3) for Tensor 'ExpandDims:0', which has shape '(1, None, None, 3)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-0666155f56c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mfid_epsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m10e-15\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfid_value_public\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_public\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_kid_given_paths\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Inception'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mdistance_public\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdistance_thresholding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_public\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Inception'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cosine_distance_eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"FID_public: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfid_value_public\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"distance_public: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdistance_public\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiplied_public: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfid_value_public\u001b[0m \u001b[1;33m/\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdistance_public\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mfid_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-4df5c0d5a9d1>\u001b[0m in \u001b[0;36mcalculate_kid_given_paths\u001b[1;34m(paths, model_name, model_path, feature_path)\u001b[0m\n\u001b[0;32m    239\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    240\u001b[0m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglobal_variables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 241\u001b[1;33m         \u001b[0mm1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_handle_path_memorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_checksize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_check_png\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    242\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfeature_path\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    243\u001b[0m             \u001b[0mm2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_handle_path_memorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_checksize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_check_png\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-4df5c0d5a9d1>\u001b[0m in \u001b[0;36m_handle_path_memorization\u001b[1;34m(path, sess, model_name, is_checksize, is_check_png)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[1;31m# In production we don't resize input images. This is just for demo purpose.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_read_checks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_checksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mimsize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_check_png\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfn\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcalculate_activation_statistics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;32mdel\u001b[0m \u001b[0mx\u001b[0m \u001b[1;31m#clean up memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-4df5c0d5a9d1>\u001b[0m in \u001b[0;36mcalculate_activation_statistics\u001b[1;34m(images, sess, model_name, batch_size, verbose)\u001b[0m\n\u001b[0;32m    203\u001b[0m                \u001b[0mthe\u001b[0m \u001b[0mincption\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    204\u001b[0m     \"\"\"\n\u001b[1;32m--> 205\u001b[1;33m     \u001b[0mact\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_activations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    206\u001b[0m     \u001b[0mmu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m     \u001b[0msigma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrowvar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-4df5c0d5a9d1>\u001b[0m in \u001b[0;36mget_activations\u001b[1;34m(images, sess, model_name, batch_size, verbose)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minception_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'input_layer'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m         \u001b[0mpred_arr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpred\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmodel_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'output_shape'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlip\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\mlip\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1162\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m   1163\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[1;32m-> 1164\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1165\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (50, 256, 256, 3) for Tensor 'ExpandDims:0', which has shape '(1, None, None, 3)'"
     ]
    }
   ],
   "source": [
    "images_path = [DATA_PATH+'photo_jpg/',DATA_PATH+'monet_jpg/']\n",
    "\n",
    "model_path = '../models/classify_image_graph_def.pb'\n",
    "\n",
    "fid_epsilon = 10e-15\n",
    "\n",
    "fid_value_public, distance_public = calculate_kid_given_paths(images_path, 'Inception', model_path)\n",
    "distance_public = distance_thresholding(distance_public, model_params['Inception']['cosine_distance_eps'])\n",
    "print(\"FID_public: \", fid_value_public, \"distance_public: \", distance_public, \"multiplied_public: \", fid_value_public /(distance_public + fid_epsilon))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Graph in module tensorflow.python.framework.ops:\n",
      "\n",
      "class Graph(builtins.object)\n",
      " |  A TensorFlow computation, represented as a dataflow graph.\n",
      " |  \n",
      " |  Graphs are used by `tf.function`s to represent the function's computations.\n",
      " |  Each graph contains a set of `tf.Operation` objects, which represent units of\n",
      " |  computation; and `tf.Tensor` objects, which represent the units of data that\n",
      " |  flow between operations.\n",
      " |  \n",
      " |  ### Using graphs directly (deprecated)\n",
      " |  \n",
      " |  A `tf.Graph` can be constructed and used directly without a `tf.function`, as\n",
      " |  was required in TensorFlow 1, but this is deprecated and it is recommended to\n",
      " |  use a `tf.function` instead. If a graph is directly used, other deprecated\n",
      " |  TensorFlow 1 classes are also required to execute the graph, such as a\n",
      " |  `tf.compat.v1.Session`.\n",
      " |  \n",
      " |  A default graph can be registered with the `tf.Graph.as_default` context\n",
      " |  manager. Then, operations will be added to the graph instead of being executed\n",
      " |  eagerly. For example:\n",
      " |  \n",
      " |  ```python\n",
      " |  g = tf.Graph()\n",
      " |  with g.as_default():\n",
      " |    # Define operations and tensors in `g`.\n",
      " |    c = tf.constant(30.0)\n",
      " |    assert c.graph is g\n",
      " |  ```\n",
      " |  \n",
      " |  `tf.compat.v1.get_default_graph()` can be used to obtain the default graph.\n",
      " |  \n",
      " |  Important note: This class *is not* thread-safe for graph construction. All\n",
      " |  operations should be created from a single thread, or external\n",
      " |  synchronization must be provided. Unless otherwise specified, all methods\n",
      " |  are not thread-safe.\n",
      " |  \n",
      " |  A `Graph` instance supports an arbitrary number of \"collections\"\n",
      " |  that are identified by name. For convenience when building a large\n",
      " |  graph, collections can store groups of related objects: for\n",
      " |  example, the `tf.Variable` uses a collection (named\n",
      " |  `tf.GraphKeys.GLOBAL_VARIABLES`) for\n",
      " |  all variables that are created during the construction of a graph. The caller\n",
      " |  may define additional collections by specifying a new name.\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self)\n",
      " |      Creates a new, empty Graph.\n",
      " |  \n",
      " |  add_to_collection(self, name, value)\n",
      " |      Stores `value` in the collection with the given `name`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class contains many\n",
      " |          standard names for collections.\n",
      " |        value: The value to add to the collection.\n",
      " |  \n",
      " |  add_to_collections(self, names, value)\n",
      " |      Stores `value` in the collections given by `names`.\n",
      " |      \n",
      " |      Note that collections are not sets, so it is possible to add a value to\n",
      " |      a collection several times. This function makes sure that duplicates in\n",
      " |      `names` are ignored, but it will not check for pre-existing membership of\n",
      " |      `value` in any of the collections in `names`.\n",
      " |      \n",
      " |      `names` can be any iterable, but if `names` is a string, it is treated as a\n",
      " |      single collection name.\n",
      " |      \n",
      " |      Args:\n",
      " |        names: The keys for the collections to add to. The `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        value: The value to add to the collections.\n",
      " |  \n",
      " |  as_default(self)\n",
      " |      Returns a context manager that makes this `Graph` the default graph.\n",
      " |      \n",
      " |      This method should be used if you want to create multiple graphs\n",
      " |      in the same process. For convenience, a global default graph is\n",
      " |      provided, and all ops will be added to this graph if you do not\n",
      " |      create a new graph explicitly.\n",
      " |      \n",
      " |      Use this method with the `with` keyword to specify that ops created within\n",
      " |      the scope of a block should be added to this graph. In this case, once\n",
      " |      the scope of the `with` is exited, the previous default graph is set again\n",
      " |      as default. There is a stack, so it's ok to have multiple nested levels\n",
      " |      of `as_default` calls.\n",
      " |      \n",
      " |      The default graph is a property of the current thread. If you\n",
      " |      create a new thread, and wish to use the default graph in that\n",
      " |      thread, you must explicitly add a `with g.as_default():` in that\n",
      " |      thread's function.\n",
      " |      \n",
      " |      The following code examples are equivalent:\n",
      " |      \n",
      " |      ```python\n",
      " |      # 1. Using Graph.as_default():\n",
      " |      g = tf.Graph()\n",
      " |      with g.as_default():\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      \n",
      " |      # 2. Constructing and making default:\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        assert c.graph is g\n",
      " |      ```\n",
      " |      \n",
      " |      If eager execution is enabled ops created under this context manager will be\n",
      " |      added to the graph instead of executed eagerly.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for using this graph as the default graph.\n",
      " |  \n",
      " |  as_graph_def(self, from_version=None, add_shapes=False)\n",
      " |      Returns a serialized `GraphDef` representation of this graph.\n",
      " |      \n",
      " |      The serialized `GraphDef` can be imported into another `Graph`\n",
      " |      (using `tf.import_graph_def`) or used with the\n",
      " |      [C++ Session API](../../api_docs/cc/index.md).\n",
      " |      \n",
      " |      This method is thread-safe.\n",
      " |      \n",
      " |      Args:\n",
      " |        from_version: Optional.  If this is set, returns a `GraphDef` containing\n",
      " |          only the nodes that were added to this graph since its `version`\n",
      " |          property had the given value.\n",
      " |        add_shapes: If true, adds an \"_output_shapes\" list attr to each node with\n",
      " |          the inferred shapes of each of its outputs.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A\n",
      " |        [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto)\n",
      " |        protocol buffer.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If the `graph_def` would be too large.\n",
      " |  \n",
      " |  as_graph_element(self, obj, allow_tensor=True, allow_operation=True)\n",
      " |      Returns the object referred to by `obj`, as an `Operation` or `Tensor`.\n",
      " |      \n",
      " |      This function validates that `obj` represents an element of this\n",
      " |      graph, and gives an informative error message if it is not.\n",
      " |      \n",
      " |      This function is the canonical way to get/validate an object of\n",
      " |      one of the allowed types from an external argument reference in the\n",
      " |      Session API.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        obj: A `Tensor`, an `Operation`, or the name of a tensor or operation. Can\n",
      " |          also be any object with an `_as_graph_element()` method that returns a\n",
      " |          value of one of these types. Note: `_as_graph_element` will be called\n",
      " |          inside the graph's lock and so may not modify the graph.\n",
      " |        allow_tensor: If true, `obj` may refer to a `Tensor`.\n",
      " |        allow_operation: If true, `obj` may refer to an `Operation`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` or `Operation` in the Graph corresponding to `obj`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `obj` is not a type we support attempting to convert\n",
      " |          to types.\n",
      " |        ValueError: If `obj` is of an appropriate type but invalid. For\n",
      " |          example, an invalid string.\n",
      " |        KeyError: If `obj` is not an object in the graph.\n",
      " |  \n",
      " |  clear_collection(self, name)\n",
      " |      Clears all values in a collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. The `GraphKeys` class contains many\n",
      " |          standard names for collections.\n",
      " |  \n",
      " |  colocate_with(self, op, ignore_existing=False)\n",
      " |      Returns a context manager that specifies an op to colocate with.\n",
      " |      \n",
      " |      Note: this function is not for public use, only for internal libraries.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      a = tf.Variable([1.0])\n",
      " |      with g.colocate_with(a):\n",
      " |        b = tf.constant(1.0)\n",
      " |        c = tf.add(a, b)\n",
      " |      ```\n",
      " |      \n",
      " |      `b` and `c` will always be colocated with `a`, no matter where `a`\n",
      " |      is eventually placed.\n",
      " |      \n",
      " |      **NOTE** Using a colocation scope resets any existing device constraints.\n",
      " |      \n",
      " |      If `op` is `None` then `ignore_existing` must be `True` and the new\n",
      " |      scope resets all colocation and device constraints.\n",
      " |      \n",
      " |      Args:\n",
      " |        op: The op to colocate all created ops with, or `None`.\n",
      " |        ignore_existing: If true, only applies colocation of this op within the\n",
      " |          context, rather than applying all colocation properties on the stack.\n",
      " |          If `op` is `None`, this value must be `True`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: if op is None but ignore_existing is False.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the op with which to colocate\n",
      " |        newly created ops.\n",
      " |  \n",
      " |  container(self, container_name)\n",
      " |      Returns a context manager that specifies the resource container to use.\n",
      " |      \n",
      " |      Stateful operations, such as variables and queues, can maintain their\n",
      " |      states on devices so that they can be shared by multiple processes.\n",
      " |      A resource container is a string name under which these stateful\n",
      " |      operations are tracked. These resources can be released or cleared\n",
      " |      with `tf.Session.reset()`.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.container('experiment0'):\n",
      " |        # All stateful Operations constructed in this context will be placed\n",
      " |        # in resource container \"experiment0\".\n",
      " |        v1 = tf.Variable([1.0])\n",
      " |        v2 = tf.Variable([2.0])\n",
      " |        with g.container(\"experiment1\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # placed in resource container \"experiment1\".\n",
      " |          v3 = tf.Variable([3.0])\n",
      " |          q1 = tf.queue.FIFOQueue(10, tf.float32)\n",
      " |        # All stateful Operations constructed in this context will be\n",
      " |        # be created in the \"experiment0\".\n",
      " |        v4 = tf.Variable([4.0])\n",
      " |        q1 = tf.queue.FIFOQueue(20, tf.float32)\n",
      " |        with g.container(\"\"):\n",
      " |          # All stateful Operations constructed in this context will be\n",
      " |          # be placed in the default resource container.\n",
      " |          v5 = tf.Variable([5.0])\n",
      " |          q3 = tf.queue.FIFOQueue(30, tf.float32)\n",
      " |      \n",
      " |      # Resets container \"experiment0\", after which the state of v1, v2, v4, q1\n",
      " |      # will become undefined (such as uninitialized).\n",
      " |      tf.Session.reset(target, [\"experiment0\"])\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        container_name: container name string.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager for defining resource containers for stateful ops,\n",
      " |          yields the container name.\n",
      " |  \n",
      " |  control_dependencies(self, control_inputs)\n",
      " |      Returns a context manager that specifies control dependencies.\n",
      " |      \n",
      " |      Use with the `with` keyword to specify that all operations constructed\n",
      " |      within the context should have control dependencies on\n",
      " |      `control_inputs`. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b, c]):\n",
      " |        # `d` and `e` will only run after `a`, `b`, and `c` have executed.\n",
      " |        d = ...\n",
      " |        e = ...\n",
      " |      ```\n",
      " |      \n",
      " |      Multiple calls to `control_dependencies()` can be nested, and in\n",
      " |      that case a new `Operation` will have control dependencies on the union\n",
      " |      of `control_inputs` from all active contexts.\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies([c, d]):\n",
      " |          # Ops constructed here run after `a`, `b`, `c`, and `d`.\n",
      " |      ```\n",
      " |      \n",
      " |      You can pass None to clear the control dependencies:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.control_dependencies([a, b]):\n",
      " |        # Ops constructed here run after `a` and `b`.\n",
      " |        with g.control_dependencies(None):\n",
      " |          # Ops constructed here run normally, not waiting for either `a` or `b`.\n",
      " |          with g.control_dependencies([c, d]):\n",
      " |            # Ops constructed here run after `c` and `d`, also not waiting\n",
      " |            # for either `a` or `b`.\n",
      " |      ```\n",
      " |      \n",
      " |      *N.B.* The control dependencies context applies *only* to ops that\n",
      " |      are constructed within the context. Merely using an op or tensor\n",
      " |      in the context does not add a control dependency. The following\n",
      " |      example illustrates this point:\n",
      " |      \n",
      " |      ```python\n",
      " |      # WRONG\n",
      " |      def my_func(pred, tensor):\n",
      " |        t = tf.matmul(tensor, tensor)\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created outside the context, so no control\n",
      " |          # dependency will be added.\n",
      " |          return t\n",
      " |      \n",
      " |      # RIGHT\n",
      " |      def my_func(pred, tensor):\n",
      " |        with tf.control_dependencies([pred]):\n",
      " |          # The matmul op is created in the context, so a control dependency\n",
      " |          # will be added.\n",
      " |          return tf.matmul(tensor, tensor)\n",
      " |      ```\n",
      " |      \n",
      " |      Also note that though execution of ops created under this scope will trigger\n",
      " |      execution of the dependencies, the ops created under this scope might still\n",
      " |      be pruned from a normal tensorflow graph. For example, in the following\n",
      " |      snippet of code the dependencies are never executed:\n",
      " |      \n",
      " |      ```python\n",
      " |        loss = model.loss()\n",
      " |        with tf.control_dependencies(dependencies):\n",
      " |          loss = loss + tf.constant(1)  # note: dependencies ignored in the\n",
      " |                                        # backward pass\n",
      " |        return tf.gradients(loss, model.variables)\n",
      " |      ```\n",
      " |      \n",
      " |      This is because evaluating the gradient graph does not require evaluating\n",
      " |      the constant(1) op created in the forward pass.\n",
      " |      \n",
      " |      Args:\n",
      " |        control_inputs: A list of `Operation` or `Tensor` objects which must be\n",
      " |          executed or computed before running the operations defined in the\n",
      " |          context.  Can also be `None` to clear the control dependencies.\n",
      " |      \n",
      " |      Returns:\n",
      " |       A context manager that specifies control dependencies for all\n",
      " |       operations constructed within the context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `control_inputs` is not a list of `Operation` or\n",
      " |          `Tensor` objects.\n",
      " |  \n",
      " |  create_op(self, op_type, inputs, dtypes=None, input_types=None, name=None, attrs=None, op_def=None, compute_shapes=True, compute_device=True)\n",
      " |      Creates an `Operation` in this graph. (deprecated arguments)\n",
      " |      \n",
      " |      Warning: SOME ARGUMENTS ARE DEPRECATED: `(compute_shapes)`. They will be removed in a future version.\n",
      " |      Instructions for updating:\n",
      " |      Shapes are always computed; don't use the compute_shapes as it has no effect.\n",
      " |      \n",
      " |      This is a low-level interface for creating an `Operation`. Most\n",
      " |      programs will not call this method directly, and instead use the\n",
      " |      Python op constructors, such as `tf.constant()`, which add ops to\n",
      " |      the default graph.\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type: The `Operation` type to create. This corresponds to the\n",
      " |          `OpDef.name` field for the proto that defines the operation.\n",
      " |        inputs: A list of `Tensor` objects that will be inputs to the `Operation`.\n",
      " |        dtypes: (Optional) A list of `DType` objects that will be the types of the\n",
      " |          tensors that the operation produces.\n",
      " |        input_types: (Optional.) A list of `DType`s that will be the types of the\n",
      " |          tensors that the operation consumes. By default, uses the base `DType`\n",
      " |          of each input in `inputs`. Operations that expect reference-typed inputs\n",
      " |          must specify `input_types` explicitly.\n",
      " |        name: (Optional.) A string name for the operation. If not specified, a\n",
      " |          name is generated based on `op_type`.\n",
      " |        attrs: (Optional.) A dictionary where the key is the attribute name (a\n",
      " |          string) and the value is the respective `attr` attribute of the\n",
      " |          `NodeDef` proto that will represent the operation (an `AttrValue`\n",
      " |          proto).\n",
      " |        op_def: (Optional.) The `OpDef` proto that describes the `op_type` that\n",
      " |          the operation will have.\n",
      " |        compute_shapes: (Optional.) Deprecated. Has no effect (shapes are always\n",
      " |          computed).\n",
      " |        compute_device: (Optional.) If True, device functions will be executed to\n",
      " |          compute the device property of the Operation.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: if any of the inputs is not a `Tensor`.\n",
      " |        ValueError: if colocation conflicts with existing device assignment.\n",
      " |      \n",
      " |      Returns:\n",
      " |        An `Operation` object.\n",
      " |  \n",
      " |  device(self, device_name_or_function)\n",
      " |      Returns a context manager that specifies the default device to use.\n",
      " |      \n",
      " |      The `device_name_or_function` argument may either be a device name\n",
      " |      string, a device function, or None:\n",
      " |      \n",
      " |      * If it is a device name string, all operations constructed in\n",
      " |        this context will be assigned to the device with that name, unless\n",
      " |        overridden by a nested `device()` context.\n",
      " |      * If it is a function, it will be treated as a function from\n",
      " |        Operation objects to device name strings, and invoked each time\n",
      " |        a new Operation is created. The Operation will be assigned to\n",
      " |        the device with the returned name.\n",
      " |      * If it is None, all `device()` invocations from the enclosing context\n",
      " |        will be ignored.\n",
      " |      \n",
      " |      For information about the valid syntax of device name strings, see\n",
      " |      the documentation in\n",
      " |      [`DeviceNameUtils`](https://www.tensorflow.org/code/tensorflow/core/util/device_name_utils.h).\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with g.device('/device:GPU:0'):\n",
      " |        # All operations constructed in this context will be placed\n",
      " |        # on GPU 0.\n",
      " |        with g.device(None):\n",
      " |          # All operations constructed in this context will have no\n",
      " |          # assigned device.\n",
      " |      \n",
      " |      # Defines a function from `Operation` to device string.\n",
      " |      def matmul_on_gpu(n):\n",
      " |        if n.type == \"MatMul\":\n",
      " |          return \"/device:GPU:0\"\n",
      " |        else:\n",
      " |          return \"/cpu:0\"\n",
      " |      \n",
      " |      with g.device(matmul_on_gpu):\n",
      " |        # All operations of type \"MatMul\" constructed in this context\n",
      " |        # will be placed on GPU 0; all other operations will be placed\n",
      " |        # on CPU 0.\n",
      " |      ```\n",
      " |      \n",
      " |      **N.B.** The device scope may be overridden by op wrappers or\n",
      " |      other library code. For example, a variable assignment op\n",
      " |      `v.assign()` must be colocated with the `tf.Variable` `v`, and\n",
      " |      incompatible device scopes will be ignored.\n",
      " |      \n",
      " |      Args:\n",
      " |        device_name_or_function: The device name or function to use in the\n",
      " |          context.\n",
      " |      \n",
      " |      Yields:\n",
      " |        A context manager that specifies the default device to use for newly\n",
      " |        created ops.\n",
      " |      \n",
      " |      Raises:\n",
      " |        RuntimeError: If device scopes are not properly nested.\n",
      " |  \n",
      " |  finalize(self)\n",
      " |      Finalizes this graph, making it read-only.\n",
      " |      \n",
      " |      After calling `g.finalize()`, no new operations can be added to\n",
      " |      `g`.  This method is used to ensure that no operations are added\n",
      " |      to a graph when it is shared between multiple threads, for example\n",
      " |      when using a `tf.compat.v1.train.QueueRunner`.\n",
      " |  \n",
      " |  get_all_collection_keys(self)\n",
      " |      Returns a list of collections used in this graph.\n",
      " |  \n",
      " |  get_collection(self, name, scope=None)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      This is different from `get_collection_ref()` which always returns the\n",
      " |      actual collection list if it exists in that it returns a new list each time\n",
      " |      it is called.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |        scope: (Optional.) A string. If supplied, the resulting list is filtered\n",
      " |          to include only items whose `name` attribute matches `scope` using\n",
      " |          `re.match`. Items without a `name` attribute are never returned if a\n",
      " |          scope is supplied. The choice of `re.match` means that a `scope` without\n",
      " |          special tokens filters by prefix.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or\n",
      " |        an empty list if no value has been added to that collection. The\n",
      " |        list contains the values in the order under which they were\n",
      " |        collected.\n",
      " |  \n",
      " |  get_collection_ref(self, name)\n",
      " |      Returns a list of values in the collection with the given `name`.\n",
      " |      \n",
      " |      If the collection exists, this returns the list itself, which can\n",
      " |      be modified in place to change the collection.  If the collection does\n",
      " |      not exist, it is created as an empty list and the list is returned.\n",
      " |      \n",
      " |      This is different from `get_collection()` which always returns a copy of\n",
      " |      the collection list if it exists and never creates an empty collection.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The key for the collection. For example, the `GraphKeys` class\n",
      " |          contains many standard names for collections.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The list of values in the collection with the given `name`, or an empty\n",
      " |        list if no value has been added to that collection.\n",
      " |  \n",
      " |  get_name_scope(self)\n",
      " |      Returns the current name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.name_scope('scope1'):\n",
      " |        with tf.name_scope('scope2'):\n",
      " |          print(tf.compat.v1.get_default_graph().get_name_scope())\n",
      " |      ```\n",
      " |      would print the string `scope1/scope2`.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string representing the current name scope.\n",
      " |  \n",
      " |  get_operation_by_name(self, name)\n",
      " |      Returns the `Operation` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Operation` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Operation` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to an operation in this graph.\n",
      " |  \n",
      " |  get_operations(self)\n",
      " |      Return the list of operations in the graph.\n",
      " |      \n",
      " |      You can modify the operations in place, but modifications\n",
      " |      to the list such as inserts/delete have no effect on the\n",
      " |      list of operations known to the graph.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A list of Operations.\n",
      " |  \n",
      " |  get_tensor_by_name(self, name)\n",
      " |      Returns the `Tensor` with the given `name`.\n",
      " |      \n",
      " |      This method may be called concurrently from multiple threads.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name of the `Tensor` to return.\n",
      " |      \n",
      " |      Returns:\n",
      " |        The `Tensor` with the given `name`.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `name` is not a string.\n",
      " |        KeyError: If `name` does not correspond to a tensor in this graph.\n",
      " |  \n",
      " |  gradient_override_map(self, op_type_map)\n",
      " |      EXPERIMENTAL: A context manager for overriding gradient functions.\n",
      " |      \n",
      " |      This context manager can be used to override the gradient function\n",
      " |      that will be used for ops within the scope of the context.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      @tf.RegisterGradient(\"CustomSquare\")\n",
      " |      def _custom_square_grad(op, grad):\n",
      " |        # ...\n",
      " |      \n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0)\n",
      " |        s_1 = tf.square(c)  # Uses the default gradient for tf.square.\n",
      " |        with g.gradient_override_map({\"Square\": \"CustomSquare\"}):\n",
      " |          s_2 = tf.square(s_2)  # Uses _custom_square_grad to compute the\n",
      " |                                # gradient of s_2.\n",
      " |      ```\n",
      " |      \n",
      " |      Args:\n",
      " |        op_type_map: A dictionary mapping op type strings to alternative op type\n",
      " |          strings.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that sets the alternative op type to be used for one\n",
      " |        or more ops created in that context.\n",
      " |      \n",
      " |      Raises:\n",
      " |        TypeError: If `op_type_map` is not a dictionary mapping strings to\n",
      " |          strings.\n",
      " |  \n",
      " |  is_feedable(self, tensor)\n",
      " |      Returns `True` if and only if `tensor` is feedable.\n",
      " |  \n",
      " |  is_fetchable(self, tensor_or_op)\n",
      " |      Returns `True` if and only if `tensor_or_op` is fetchable.\n",
      " |  \n",
      " |  name_scope(self, name)\n",
      " |      Returns a context manager that creates hierarchical names for operations.\n",
      " |      \n",
      " |      A graph maintains a stack of name scopes. A `with name_scope(...):`\n",
      " |      statement pushes a new name onto the stack for the lifetime of the context.\n",
      " |      \n",
      " |      The `name` argument will be interpreted as follows:\n",
      " |      \n",
      " |      * A string (not ending with '/') will create a new name scope, in which\n",
      " |        `name` is appended to the prefix of all operations created in the\n",
      " |        context. If `name` has been used before, it will be made unique by\n",
      " |        calling `self.unique_name(name)`.\n",
      " |      * A scope previously captured from a `with g.name_scope(...) as\n",
      " |        scope:` statement will be treated as an \"absolute\" name scope, which\n",
      " |        makes it possible to re-enter existing scopes.\n",
      " |      * A value of `None` or the empty string will reset the current name scope\n",
      " |        to the top-level (empty) name scope.\n",
      " |      \n",
      " |      For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      with tf.Graph().as_default() as g:\n",
      " |        c = tf.constant(5.0, name=\"c\")\n",
      " |        assert c.op.name == \"c\"\n",
      " |        c_1 = tf.constant(6.0, name=\"c\")\n",
      " |        assert c_1.op.name == \"c_1\"\n",
      " |      \n",
      " |        # Creates a scope called \"nested\"\n",
      " |        with g.name_scope(\"nested\") as scope:\n",
      " |          nested_c = tf.constant(10.0, name=\"c\")\n",
      " |          assert nested_c.op.name == \"nested/c\"\n",
      " |      \n",
      " |          # Creates a nested scope called \"inner\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_c = tf.constant(20.0, name=\"c\")\n",
      " |            assert nested_inner_c.op.name == \"nested/inner/c\"\n",
      " |      \n",
      " |          # Create a nested scope called \"inner_1\".\n",
      " |          with g.name_scope(\"inner\"):\n",
      " |            nested_inner_1_c = tf.constant(30.0, name=\"c\")\n",
      " |            assert nested_inner_1_c.op.name == \"nested/inner_1/c\"\n",
      " |      \n",
      " |            # Treats `scope` as an absolute name scope, and\n",
      " |            # switches to the \"nested/\" scope.\n",
      " |            with g.name_scope(scope):\n",
      " |              nested_d = tf.constant(40.0, name=\"d\")\n",
      " |              assert nested_d.op.name == \"nested/d\"\n",
      " |      \n",
      " |              with g.name_scope(\"\"):\n",
      " |                e = tf.constant(50.0, name=\"e\")\n",
      " |                assert e.op.name == \"e\"\n",
      " |      ```\n",
      " |      \n",
      " |      The name of the scope itself can be captured by `with\n",
      " |      g.name_scope(...) as scope:`, which stores the name of the scope\n",
      " |      in the variable `scope`. This value can be used to name an\n",
      " |      operation that represents the overall result of executing the ops\n",
      " |      in a scope. For example:\n",
      " |      \n",
      " |      ```python\n",
      " |      inputs = tf.constant(...)\n",
      " |      with g.name_scope('my_layer') as scope:\n",
      " |        weights = tf.Variable(..., name=\"weights\")\n",
      " |        biases = tf.Variable(..., name=\"biases\")\n",
      " |        affine = tf.matmul(inputs, weights) + biases\n",
      " |        output = tf.nn.relu(affine, name=scope)\n",
      " |      ```\n",
      " |      \n",
      " |      NOTE: This constructor validates the given `name`. Valid scope\n",
      " |      names match one of the following regular expressions:\n",
      " |      \n",
      " |          [A-Za-z0-9.][A-Za-z0-9_.\\-/]* (for scopes at the root)\n",
      " |          [A-Za-z0-9_.\\-/]* (for other scopes)\n",
      " |      \n",
      " |      Args:\n",
      " |        name: A name for the scope.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A context manager that installs `name` as a new name scope.\n",
      " |      \n",
      " |      Raises:\n",
      " |        ValueError: If `name` is not a valid scope name, according to the rules\n",
      " |          above.\n",
      " |  \n",
      " |  prevent_feeding(self, tensor)\n",
      " |      Marks the given `tensor` as unfeedable in this graph.\n",
      " |  \n",
      " |  prevent_fetching(self, op)\n",
      " |      Marks the given `op` as unfetchable in this graph.\n",
      " |  \n",
      " |  switch_to_thread_local(self)\n",
      " |      Make device, colocation and dependencies stacks thread-local.\n",
      " |      \n",
      " |      Device, colocation and dependencies stacks are not thread-local be default.\n",
      " |      If multiple threads access them, then the state is shared.  This means that\n",
      " |      one thread may affect the behavior of another thread.\n",
      " |      \n",
      " |      After this method is called, the stacks become thread-local.  If multiple\n",
      " |      threads access them, then the state is not shared.  Each thread uses its own\n",
      " |      value; a thread doesn't affect other threads by mutating such a stack.\n",
      " |      \n",
      " |      The initial value for every thread's stack is set to the current value\n",
      " |      of the stack when `switch_to_thread_local()` was first called.\n",
      " |  \n",
      " |  unique_name(self, name, mark_as_used=True)\n",
      " |      Return a unique operation name for `name`.\n",
      " |      \n",
      " |      Note: You rarely need to call `unique_name()` directly.  Most of\n",
      " |      the time you just need to create `with g.name_scope()` blocks to\n",
      " |      generate structured names.\n",
      " |      \n",
      " |      `unique_name` is used to generate structured names, separated by\n",
      " |      `\"/\"`, to help identify operations when debugging a graph.\n",
      " |      Operation names are displayed in error messages reported by the\n",
      " |      TensorFlow runtime, and in various visualization tools such as\n",
      " |      TensorBoard.\n",
      " |      \n",
      " |      If `mark_as_used` is set to `True`, which is the default, a new\n",
      " |      unique name is created and marked as in use. If it's set to `False`,\n",
      " |      the unique name is returned without actually being marked as used.\n",
      " |      This is useful when the caller simply wants to know what the name\n",
      " |      to be created will be.\n",
      " |      \n",
      " |      Args:\n",
      " |        name: The name for an operation.\n",
      " |        mark_as_used: Whether to mark this name as being used.\n",
      " |      \n",
      " |      Returns:\n",
      " |        A string to be passed to `create_op()` that will be used\n",
      " |        to name the operation being created.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |  \n",
      " |  building_function\n",
      " |      Returns True iff this graph represents a function.\n",
      " |  \n",
      " |  collections\n",
      " |      Returns the names of the collections known to this graph.\n",
      " |  \n",
      " |  finalized\n",
      " |      True if this graph has been finalized.\n",
      " |  \n",
      " |  graph_def_versions\n",
      " |      The GraphDef version information of this graph.\n",
      " |      \n",
      " |      For details on the meaning of each version, see\n",
      " |      [`GraphDef`](https://www.tensorflow.org/code/tensorflow/core/framework/graph.proto).\n",
      " |      \n",
      " |      Returns:\n",
      " |        A `VersionDef`.\n",
      " |  \n",
      " |  version\n",
      " |      Returns a version number that increases as ops are added to the graph.\n",
      " |      \n",
      " |      Note that this is unrelated to the\n",
      " |      `tf.Graph.graph_def_versions`.\n",
      " |      \n",
      " |      Returns:\n",
      " |         An integer version that increases as ops are added to the graph.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  seed\n",
      " |      The graph-level random seed of this graph.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(tf.Graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
